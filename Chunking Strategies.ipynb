{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21de2eed-24ad-4576-8ca7-8cd451de6be8",
   "metadata": {},
   "source": [
    "# The Hound of Baskervilles\n",
    "https://www.gutenberg.org/cache/epub/3070/pg3070.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c4fb2d-eca7-46eb-9ef8-4608151fe730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b1c2cf-8e5d-4c3e-ac99-e627f4a1b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/cache/epub/3070/pg3070.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a69cd24-7947-45e8-8618-91282689a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c961d43-322b-481e-a248-67495abce55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if response.status_code == 200:\n",
    "    book_full_text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f499069-5b28-4101-bdc1-a3b6459c4ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d282cf1-20c7-45fc-88ab-b1ff40503f29",
   "metadata": {},
   "source": [
    "## Calculating tokens using tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a920777b-f312-4f2c-9c34-8374563ff039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiktoken import encoding_for_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae06b4-82de-4530-bcb6-3493ef6e9e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoding_for_model(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a37ef7-f4a1-49cb-b328-cb2bee4d9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = encoder.encode(book_full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdeab39-4ce0-4ee6-9ac4-2205ffff1cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90ad63b-bef9-400a-8e55-e82b65576d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9ad9a6f-f6a6-432c-89cb-c5c85cc79c85",
   "metadata": {},
   "source": [
    "## Fixed Window Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53464e1c-fb5d-4852-9d6a-25f623ae9c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13bae8-2d70-48fd-9b02-11015ca57f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_splitter_v1(text: str) -> List[str]:\n",
    "    \"\"\"Splits text at every new line\"\"\"\n",
    "    return text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c584c55e-67e7-4399-b33b-696f8ea0e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_splitter_v2(text: str, separators: List[str] = [\"\\n\", \"\\r\"]) -> List[str]:\n",
    "    \"\"\"Splits text at every separator\"\"\"\n",
    "    splits = [text]\n",
    "    for sep in separators:\n",
    "        splits = [segment for part in splits for segment in part.split(sep) if segment]\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1a6cd8-8c6a-488c-b11c-9a7e27969220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_window_splitter(text: str, chunk_size: int = 1000) -> List[str]:\n",
    "    \"\"\"Splits text at given chunk_size\"\"\"\n",
    "    splits = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        splits.append(text[i:i + chunk_size])\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f2022-0f03-41e6-acc2-9bbcc8d48819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_window_with_overlap_splitter(text: str, chunk_size: int = 1000, chunk_overlap: int = 10) -> List[str]:\n",
    "    \"\"\"Splits text at given chunk_size, and starts next chunk from start - chunk_overlap position\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start <= len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c190d-9812-4c0f-ad42-ff57788c03e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_character_splitter(text: str, separators: List[str] = [\"\\n\", \"\\r\"], chunk_size: int = 1000, chunk_overlap: int = 10) -> List[str]:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    splitter = RecursiveCharacterTextSplitter(separators=separators, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6607819a-84f7-4397-b2e3-9e780bddc524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_splitter(text: str, chunk_size: int = 1000, chunk_overlap: int = 10) -> List[str]:\n",
    "    from flair.models import SequenceTagger\n",
    "    from flair.data import Sentence\n",
    "    from flair.splitter import SegtokSentenceSplitter\n",
    "\n",
    "    splitter = SegtokSentenceSplitter()\n",
    "    \n",
    "    # Split text into sentences\n",
    "    sentences = splitter.split(text)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Add sentence to the current chunk\n",
    "        if len(current_chunk) + len(sentence.to_plain_string()) <= chunk_size:\n",
    "            current_chunk += \" \" + sentence.to_plain_string()\n",
    "        else:\n",
    "            # If adding the next sentence exceeds max size, start a new chunk\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence.to_plain_string()\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f63e35-356f-4447-b7f3-790837bf75da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.splitter import SegtokSentenceSplitter\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Placeholder function to generate embeddings using your own OpenAI model.\n",
    "def get_embedding(sentence: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a given sentence using your custom OpenAI model.\n",
    "    Replace this with your actual model API call.\n",
    "    \"\"\"\n",
    "    # Example placeholder; replace with actual API call, e.g.:\n",
    "    model = \"text-embedding-ada-002-01\"\n",
    "    api_version = \"2023-05-15\"\n",
    "\n",
    "    embeddings = AzureOpenAIEmbeddings(model=model,\n",
    "                                 api_version=api_version,\n",
    "                                 azure_endpoint=os.getenv(\n",
    "                                     \"AZURE_OPENAI_ENDPOINT\"),\n",
    "                                 api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"))\n",
    "    return embeddings.embed_query(sentence)\n",
    "\n",
    "\n",
    "def cosine_sim(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]\n",
    "\n",
    "def can_merge(sentences: List[str], chunk_size: int, similarity_threshold: float) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the group of sentences can be merged into a chunk without exceeding chunk_size\n",
    "    and meeting the similarity criteria.\n",
    "    \"\"\"\n",
    "    combined_text = \" \".join(sentences)\n",
    "    if len(combined_text) > chunk_size:\n",
    "        return False\n",
    "\n",
    "    # Get embeddings for each sentence\n",
    "    embeddings = [get_embedding(sentence) for sentence in sentences]\n",
    "\n",
    "    # Check similarity between consecutive sentences\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        if cosine_sim(embeddings[i], embeddings[i + 1]) < similarity_threshold:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def recursive_merge(sentences: List[str], chunk_size: int = 1000, similarity_threshold: float = 0.8) -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively merge sentences into meaningful chunks based on similarity and chunk size constraints.\n",
    "    \n",
    "    Args:\n",
    "        sentences (List[str]): List of sentences to be merged.\n",
    "        chunk_size (int): Maximum size of each chunk in characters.\n",
    "        similarity_threshold (float): Minimum similarity score to allow merging.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: A list of merged semantic chunks.\n",
    "    \"\"\"\n",
    "    if not sentences:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        current_chunk.append(sentence)\n",
    "        \n",
    "        # Check if the current chunk can be merged based on size and similarity\n",
    "        if can_merge(current_chunk, chunk_size, similarity_threshold):\n",
    "            continue\n",
    "        else:\n",
    "            # If merging is not possible, finalize the current chunk without the last sentence\n",
    "            if len(current_chunk) > 1:\n",
    "                chunks.append(\" \".join(current_chunk[:-1]))\n",
    "                current_chunk = [current_chunk[-1]]  # Start new chunk with the last sentence\n",
    "\n",
    "    # Add the last remaining chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def embedding_chunking(text: str, chunk_size: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits the text into semantically meaningful chunks based on embeddings and chunk size constraints.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to be split.\n",
    "        chunk_size (int): Maximum size of each chunk in characters.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: A list of semantic chunks.\n",
    "    \"\"\"\n",
    "    # Initialize the sentence splitter from Flair\n",
    "    splitter = SegtokSentenceSplitter()\n",
    "    \n",
    "    # Split the text into sentences\n",
    "    sentences = splitter.split(text)\n",
    "    \n",
    "    # Convert Flair Sentence objects to plain strings\n",
    "    sentence_texts = [sentence.to_plain_string() for sentence in sentences]\n",
    "    \n",
    "    # Call the recursive merge function to create chunks\n",
    "    chunks = recursive_merge(sentence_texts, chunk_size=chunk_size, similarity_threshold=0.5)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "text = (book_full_text\n",
    ")\n",
    "\n",
    "display(embedding_chunking(text, chunk_size=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3973e3-69ea-4649-befe-33f2f610061d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd377b-ee1c-4f5f-9891-9dd78d684ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf0ee8a-6b32-4cd1-9304-578eaa5ae0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(items: List[str]):\n",
    "    for item in items:\n",
    "        print(f\"|{item}|\")\n",
    "        print(\"=========END OF CHUNK===========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e912b15-b35c-418e-9623-21d2a09b9803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(fixed_window_with_overlap_splitter(book_full_text, chunk_size=400, chunk_overlap=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d2acf0-6aad-4000-9025-14b13ff8a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_character_splitter(book_full_text, separators=[\"\\n\\n\", \"\\n\\r\", \"\\n\", \"\\r\"], chunk_size=400, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138cb7a5-968c-4451-98e6-30fbe06160ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_splitter(book_full_text, chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4327b9ed-fe0a-4860-8389-60eea44dc608",
   "metadata": {},
   "source": [
    "## Embedding Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad12155-f100-42da-a29a-d4396cb72b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_splitter(text_data, chunk_size=400):\n",
    "    import os\n",
    "    import nltk\n",
    "    from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import numpy as np\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "    from tqdm import tqdm\n",
    "    from flair.splitter import SegtokSentenceSplitter\n",
    "    \n",
    "    load_dotenv(find_dotenv())\n",
    "    \n",
    "    \n",
    "    # Set Azure OpenAI API environment variables (ensure these are set in your environment)\n",
    "    # You can also set these in your environment directly\n",
    "    # os.environ[\"OPENAI_API_KEY\"] = \"your-azure-openai-api-key\"\n",
    "    # os.environ[\"OPENAI_API_BASE\"] = \"your-azure-openai-api-endpoint\"\n",
    "    os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "    \n",
    "    # Initialize OpenAIEmbeddings using LangChain's Azure support\n",
    "    embedding_model = AzureOpenAIEmbeddings(deployment=\"text-embedding-ada-002-01\")  # Use your Azure model name\n",
    "    \n",
    "    \n",
    "    # Step 1: Split the text into sentences\n",
    "    def split_into_sentences(text):\n",
    "        splitter = SegtokSentenceSplitter()\n",
    "    \n",
    "        # Split text into sentences\n",
    "        sentences = splitter.split(text)\n",
    "        sentence_str = []\n",
    "        for sentence in sentences:\n",
    "            sentence_str.append(sentence.to_plain_string())\n",
    "        return sentence_str[:100]\n",
    "    \n",
    "    # Step 2: Get embeddings for each sentence using the same Azure embedding model\n",
    "    def get_embeddings(sentences):\n",
    "        embeddings = []\n",
    "        for sentence in tqdm(sentences, desc=\"Generating embeddings\"):\n",
    "            embedding = embedding_model.embed_documents([sentence])  # Embeds a single sentence\n",
    "            embeddings.append(embedding[0])  # embed_documents returns a list, so take the first element\n",
    "        return embeddings\n",
    "    \n",
    "    # Step 3: Form chunks based on sentence embeddings, a similarity threshold, and a max chunk character size\n",
    "    def form_chunks(sentences, embeddings, similarity_threshold=0.7, chunk_size=500):\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_chunk_emb = []\n",
    "        current_chunk_length = 0  # Track the character length of the current chunk\n",
    "    \n",
    "        for i, (sentence, emb) in enumerate(zip(sentences, embeddings)):\n",
    "            emb = np.array(emb)  # Ensure the embedding is a numpy array\n",
    "            sentence_length = len(sentence)  # Calculate the length of the sentence\n",
    "    \n",
    "            if current_chunk:\n",
    "                # Calculate similarity with the current chunk's embedding (mean of embeddings in the chunk)\n",
    "                chunk_emb = np.mean(np.array(current_chunk_emb), axis=0).reshape(1, -1)  # Average embedding of the chunk\n",
    "                similarity = cosine_similarity(emb.reshape(1, -1), chunk_emb)[0][0]\n",
    "    \n",
    "                if similarity < similarity_threshold or current_chunk_length + sentence_length > chunk_size:\n",
    "                    # If similarity is below threshold or adding this sentence exceeds max chunk size, create a new chunk\n",
    "                    chunks.append(current_chunk)\n",
    "                    current_chunk = [sentence]\n",
    "                    current_chunk_emb = [emb]\n",
    "                    current_chunk_length = sentence_length  # Reset chunk length\n",
    "                else:\n",
    "                    # Else, add sentence to the current chunk\n",
    "                    current_chunk.append(sentence)\n",
    "                    current_chunk_emb.append(emb)\n",
    "                    current_chunk_length += sentence_length  # Update chunk length\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_chunk_emb = [emb]\n",
    "                current_chunk_length = sentence_length  # Set initial chunk length\n",
    "    \n",
    "        # Add the last chunk\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "    \n",
    "        return chunks\n",
    "    \n",
    "    # Apply the sentence splitting\n",
    "    sentences = split_into_sentences(text_data)\n",
    "    \n",
    "    # Get sentence embeddings\n",
    "    embeddings = get_embeddings(sentences)\n",
    "    \n",
    "    # Form chunks based on embeddings\n",
    "    chunks = form_chunks(sentences, embeddings, chunk_size=chunk_size)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d94c8d-6799-49f8-9e14-fb8e87dd17ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = embedding_splitter(book_full_text, chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6fbc40-dfed-494a-8b35-c6388d9bb620",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efab720f-9441-4ec0-a781-c365e46ed352",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f9572-d4ea-4b43-a516-64dd0cc4f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentic_chunking(text_data):\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    llm = AzureChatOpenAI(model=\"gpt-4o\",\n",
    "                           azure_endpoint=os.getenv(\n",
    "                               \"AZURE_OPENAI_ENDPOINT\"),\n",
    "                           api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "                           api_version=\"2023-03-15-preview\",\n",
    "                           verbose=True,\n",
    "                           temperature=1)\n",
    "    prompt = \"\"\"I am providing a document below. \n",
    "    Please split the document into chunks that maintain semantic coherence and ensure that each chunk represents a complete and meaningful unit of information. \n",
    "    Each chunk should stand alone, preserving the context and meaning without splitting key ideas across chunks. \n",
    "    Use your understanding of the content’s structure, topics, and flow to identify natural breakpoints in the text. \n",
    "    Ensure that no chunk exceeds 1000 characters length, and prioritize keeping related concepts or sections together.\n",
    "\n",
    "    Do not modify the document, just split to chunks and return them as an array of strings, where each string is one chunk of the document.\n",
    "    Return the entire book not dont stop in betweek some sentences.\n",
    "\n",
    "    Document:\n",
    "    {document}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate.from_template(prompt)\n",
    "\n",
    "    chain = prompt_template | llm\n",
    "\n",
    "    result = chain.invoke({\"document\": text_data})\n",
    "    return result\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f185c2-3fb2-431c-88f6-7bbbbf5f47bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agentic_chunking(book_full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8f7e46-8b00-4ddc-97bf-0d1814b9cd28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
